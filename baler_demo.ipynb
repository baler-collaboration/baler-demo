{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Baler - Machine Learning Based Data Compression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Probelm\n",
    "* “Data deluge”: when the sheer volume of new data being generated is overwhelming the storage and processing capacities\n",
    "* An increasingly common problem in both academia and industry.\n",
    "<div>\n",
    "<img src=\"figures/data_deluge.png\" width=\"750\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Solution\n",
    "* One approach: Lossy compression (Like mp3 for music)\n",
    "* One catch: Lossy compression needs to be tailored\n",
    "* Solution: Lossy Machine Learning based compression\n",
    "<div>\n",
    "<img src=\"figures/AE.png\" width=\"900\"/>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Our Tool: “Baler”\n",
    "* We have created a tool called “Baler” to help investigate the viability of this compression\n",
    "* Multidisciplinary tool, easy to install and run as a python pachage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eX0s7ygpFerQ",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -i https://test.pypi.org/simple/ baler-compressor\n",
    "    \n",
    "import baler_compressor.config as config_module\n",
    "import baler_compressor.trainer as trainer_module\n",
    "import baler_compressor.compressor as compressor_module\n",
    "import baler_compressor.decompressor as decompressor_module\n",
    "\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "* Distributed and developed as an open source project\n",
    "    - https://github.com/baler-collaboration/baler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## X-ray Tomogapy Input Data (exafel_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NP7RynYGKXDh",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "data = np.load(\"input/exafel_1.npz\")[\"data\"]\n",
    "data_decompressed = data*0\n",
    "\n",
    "helper.plot2D(data[0], data_decompressed[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Baler Workflow\n",
    "<div>\n",
    "<img src=\"figures/workflow.png\" width=\"800\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Define Training Parameters (config file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1TzrUj-SFwe3",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "config = config_module.Config\n",
    "config.compression_ratio = 100\n",
    "config.epochs = 1\n",
    "config.early_stopping = False\n",
    "config.early_stopping_patience = 100\n",
    "config.min_delta = 0\n",
    "config.lr_scheduler = True\n",
    "config.lr_scheduler_patience = 50\n",
    "config.model_name = \"CFD_dense_AE\"\n",
    "config.model_type = \"dense\"\n",
    "config.custom_norm = True\n",
    "config.l1 = True\n",
    "config.reg_param = 0.001\n",
    "config.RHO = 0.05\n",
    "config.lr = 0.001\n",
    "config.batch_size = 5\n",
    "config.test_size = 0.2\n",
    "config.data_dimension = 2\n",
    "config.apply_normalization = False\n",
    "config.deterministic_algorithm = False\n",
    "config.compress_to_latent_space = False\n",
    "config.convert_to_blocks = [1, 150, 150]\n",
    "config.verbose = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Run Training\n",
    "* Arguments:\n",
    "    - Input data path\n",
    "    - Config class instance\n",
    "* Returns:\n",
    "    - Trained pyTorch model\n",
    "    - Normalization features\n",
    "    - Loss data (training/validation vs. epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-jgQSjD1F87N",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "input_data_path = \"/home/pekman/baler-demo/input/exafel_1.npz\"\n",
    "output_path = \"/home/pekman/baler-demo/output/\"\n",
    "\n",
    "model, normalization_features, loss_data = trainer_module.run(input_data_path, config)\n",
    "torch.save(model.state_dict(), output_path + \"compressed_output/model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Running Compression\n",
    "* Compression function takes:\n",
    "    - Input data path\n",
    "    - Input model path\n",
    "    - Normalization features\n",
    "    - Config\n",
    "* returns\n",
    "    - Compressed data (numpy array)\n",
    "    - Column names (If tabular data)\n",
    "    - Original data shape\n",
    "    - Normalization features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eNgJNvKnF_gl",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "compressed, names, normalization_features, original_shape = compressor_module.run(\n",
    "    input_data_path,\n",
    "    output_path + \"compressed_output/model_preBaked.pt\",\n",
    "    normalization_features,\n",
    "    config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Saving compressed data to disk\n",
    "* We can save the returns to disk as the decompressed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "np.savez_compressed(\n",
    "    output_path + \"compressed_output/compressed.npz\",\n",
    "    data=compressed,\n",
    "    names=names,\n",
    "    normalization_features=normalization_features,\n",
    "    original_shape=original_shape,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Running Decompression\n",
    "* Decompression function takes:\n",
    "    - Input model path\n",
    "    - Input compressed data path\n",
    "    - Config\n",
    "* returns\n",
    "    - Deompressed data (numpy array)\n",
    "    - Column names (If tabular data)\n",
    "    - Original data shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k3bh5qh9GWhm",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "decompressed, names, original_shape = decompressor_module.run(\n",
    "    output_path + \"compressed_output/model_preBaked.pt\",\n",
    "    output_path + \"compressed_output/compressed.npz\",\n",
    "    config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Saving decompressed data  to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "np.savez(\n",
    "    output_path + \"decompressed_output/decompressed.npz\",\n",
    "    data=decompressed,\n",
    "    names=names,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluating performance\n",
    "* Use extrenal plotting to look at performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rKqlONa5J8D3",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "data = np.load(\"input/exafel_1.npz\")[\"data\"]\n",
    "data_decompressed = np.load(\"output/decompressed_output/decompressed.npz\")[\"data\"].reshape(data.shape[0], data.shape[1], data.shape[2])\n",
    "helper.plot2D(data[0], data_decompressed[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Offline vs Online compression\n",
    "* Offline compression is always done on data which is fully aquired\n",
    "    - A single model is then derived for that one dataset\n",
    "    - Auxilliary files like normalization factors and the model need to be included in the compressed file size\n",
    "* Online compression can be done on data in real time\n",
    "    - Uses a model derived on similar, previously aquired datasets\n",
    "    - Auxilliary files do not count towards final compressed file size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Demonstrating online compression\n",
    "* New, but similar, dataset \"exafel_2\"\n",
    "* Same model as was derived from \"exafel_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SmvckbKLMkQv",
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Run compression\n",
    "compressed, names, normalization_features, original_shape = compressor_module.run(\n",
    "    \"input/exafel_2.npz\",\n",
    "    output_path + \"compressed_output/model_preBaked.pt\",\n",
    "    normalization_features,\n",
    "    config,\n",
    ")\n",
    "# Save compressed file\n",
    "np.savez_compressed(\n",
    "    output_path + \"compressed_output/compressed2.npz\",\n",
    "    data=compressed,\n",
    "    names=names,\n",
    "    normalization_features=normalization_features,\n",
    "    original_shape=original_shape,\n",
    ")\n",
    "# Run decompression\n",
    "decompressed, names, original_shape = decompressor_module.run(\n",
    "    output_path + \"compressed_output/model_preBaked.pt\",\n",
    "    output_path + \"compressed_output/compressed2.npz\",\n",
    "    config,\n",
    ")\n",
    "# Save decompressed file\n",
    "np.savez(\n",
    "    output_path + \"decompressed_output/decompressed2.npz\",\n",
    "    data=decompressed,\n",
    "    names=names,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluation of Online compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q7HAxWnwSgFD",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "data = np.load(\"input/exafel_2.npz\")[\"data\"]\n",
    "data_decompressed = np.load(\"output/decompressed_output/decompressed2.npz\")[\"data\"].reshape(data.shape[0], data.shape[1], data.shape[2])\n",
    "helper.plot2D(data[10], data_decompressed[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outlook\n",
    "* The interest for machine learning based offline compresion is limited\n",
    "* The interest for online compression is unlimited?\n",
    "    - Telecom\n",
    "    - Autonomous vehicles\n",
    "    - Detector readout\n",
    "    - Anything with bandwidth constraints"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Slideshow",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
